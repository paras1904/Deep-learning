import numpy as np
from random import randint
from sklearn.preprocessing import MinMaxScaler

train_samples = []
train_labels = []

for i in range(1000):
    random_younger = randint(13,64)
    train_samples.append(random_younger)
    train_labels.append(0)

    random_older = randint(65,100)
    train_samples.append(random_older)
    train_labels.append(1)

for i in range(50):
    random_younger = randint(13,64)
    train_samples.append(random_younger)
    train_labels.append(1)

    random_older = randint(65,100)
    train_samples.append(random_older)
    train_labels.append(0)

# fake data display
# for i in range(1050):
    # print(f"{train_samples[i]} - {train_labels[i]}")

# convert random data into numpy array
train_samples = np.array(train_samples)
train_labels = np.array(train_labels)

# scaling the value 13 to 64
scaler = MinMaxScaler(feature_range=(0,1))
scaler_train_samples = scaler.fit_transform((train_samples).reshape(-1,1))
print(scaler_train_samples)

import keras
from keras import backend as k
from keras.models import Sequential
from keras import layers
from keras.layers.core import Dense
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy

model = Sequential([
    Dense(16,input_dim=1,activation='relu'),
    Dense(32,activation='relu'),
    Dense(2,activation='softmax')
])
print(model.summary())
model.compile(Adam(lr=0.0001),loss="sparse_categorical_crossentropy",metrics=['accuracy'])
model.fit(scaler_train_samples,train_labels,batch_size=10,epochs=10,shuffle=True,verbose=2)
