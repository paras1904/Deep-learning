# -*- coding: utf-8 -*-
"""Text classification with bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10C4DCSnG4E9EJz-98BrMiXLHYMXHquBN
"""

# dataset_link -> https://www.kaggle.com/uciml/sms-spam-collection-dataset/home

!python -m pip install tensorflow_text
import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

df = pd.read_csv('/content/spam.csv', encoding="ISO-8859-1")
df.head()

df = df[['v1','v2']]
df=df.dropna()
df.head()

def func1(s):
  s = s.lower()
  if s=='spam':
    return 1
  else:
    return 0
df['v1'] = df['v1'].apply(lambda x:func1(x))

df['v1'].value_counts()

# balencing the unbalenced data
ham = df[df['v1']==0]
ham = ham.sample(747)
spam = df[df['v1']==1]
del df
df = pd.concat([spam,ham],axis=0)
df.head()

from sklearn.model_selection import train_test_split
x_train,x_test, y_train,y_test = train_test_split(df['v2'],df['v1'],stratify=df['v1'])

bert_preprocess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")

def sent_embd(s):
  preproc = bert_preprocess(s)
  return bert_encoder(preproc)['pooled_output']

# model
# bert_layers
text_input = tf.keras.layers.Input(shape=(),dtype=tf.string,name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

l = tf.keras.layers.Dropout(0.1,name='dropout')(outputs['pooled_output'])
l = tf.keras.layers.Dense(1,activation='sigmoid',name='output')(l)

model = tf.keras.Model(inputs = [text_input],outputs=[l])

model.summary()

Met=[
     tf.keras.metrics.BinaryAccuracy(name='accuracy'),
     tf.keras.metrics.Precision(name='precision'),
     tf.keras.metrics.Recall(name='recall')
]

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=Met)

model.fit(x_train,y_train,epochs=10)

model.evaluate(x_test, y_test)